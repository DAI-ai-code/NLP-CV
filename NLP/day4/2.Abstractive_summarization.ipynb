{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e85c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl (277 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ---------------------------------------- 0/6 [tqdm]\n",
      "   ------------- -------------------------- 2/6 [regex]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------------- ------------- 4/6 [tokenizers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   ---------------------------------------- 6/6 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3\n",
      "\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl (277 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ---------------------------------------- 0/6 [tqdm]\n",
      "   ------------- -------------------------- 2/6 [regex]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------------- ------------- 4/6 [tokenizers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   --------------------------------- ------ 5/6 [transformers]\n",
      "   ---------------------------------------- 6/6 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57f02bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4351ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pipeline in module transformers.pipelines:\n",
      "\n",
      "pipeline(task: Optional[str] = None, model: Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None, feature_extractor: Union[ForwardRef('SequenceFeatureExtractor'), str, NoneType] = None, image_processor: Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None, processor: Union[str, transformers.processing_utils.ProcessorMixin, NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Union[str, bool, NoneType] = None, device: Union[int, str, ForwardRef('torch.device'), NoneType] = None, device_map: Union[str, dict[str, Union[int, str]], NoneType] = None, dtype: Union[str, ForwardRef('torch.dtype'), NoneType] = 'auto', trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> transformers.pipelines.base.Pipeline\n",
      "    Utility factory method to build a [`Pipeline`].\n",
      "    \n",
      "    A pipeline consists of:\n",
      "    \n",
      "        - One or more components for pre-processing model inputs, such as a [tokenizer](tokenizer),\n",
      "        [image_processor](image_processor), [feature_extractor](feature_extractor), or [processor](processors).\n",
      "        - A [model](model) that generates predictions from the inputs.\n",
      "        - Optional post-processing steps to refine the model's output, which can also be handled by processors.\n",
      "    \n",
      "    <Tip>\n",
      "    While there are such optional arguments as `tokenizer`, `feature_extractor`, `image_processor`, and `processor`,\n",
      "    they shouldn't be specified all at once. If these components are not provided, `pipeline` will try to load\n",
      "    required ones automatically. In case you want to provide these components explicitly, please refer to a\n",
      "    specific pipeline in order to get more details regarding what components are required.\n",
      "    </Tip>\n",
      "    \n",
      "    Args:\n",
      "        task (`str`):\n",
      "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
      "    \n",
      "            - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
      "            - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
      "            - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
      "            - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
      "            - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
      "            - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
      "            - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
      "            - `\"image-feature-extraction\"`: will return an [`ImageFeatureExtractionPipeline`].\n",
      "            - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
      "            - `\"image-text-to-text\"`: will return a [`ImageTextToTextPipeline`].\n",
      "            - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n",
      "            - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
      "            - `\"keypoint-matching\"`: will return a [`KeypointMatchingPipeline`].\n",
      "            - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
      "            - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
      "            - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
      "            - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
      "            - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
      "            - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
      "            - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
      "              [`TextClassificationPipeline`].\n",
      "            - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
      "            - `\"text-to-audio\"` (alias `\"text-to-speech\"` available): will return a [`TextToAudioPipeline`]:.\n",
      "            - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
      "            - `\"translation\"`: will return a [`TranslationPipeline`].\n",
      "            - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
      "            - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
      "            - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
      "            - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
      "            - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
      "            - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
      "            - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`].\n",
      "    \n",
      "        model (`str` or [`PreTrainedModel`] or [`TFPreTrainedModel`], *optional*):\n",
      "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
      "            actual instance of a pretrained model inheriting from [`PreTrainedModel`] (for PyTorch) or\n",
      "            [`TFPreTrainedModel`] (for TensorFlow).\n",
      "    \n",
      "            If not provided, the default for the `task` will be loaded.\n",
      "        config (`str` or [`PretrainedConfig`], *optional*):\n",
      "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
      "            identifier or an actual pretrained model configuration inheriting from [`PretrainedConfig`].\n",
      "    \n",
      "            If not provided, the default configuration file for the requested model will be used. That means that if\n",
      "            `model` is given, its default configuration will be used. However, if `model` is not supplied, this\n",
      "            `task`'s default model's config is used instead.\n",
      "        tokenizer (`str` or [`PreTrainedTokenizer`], *optional*):\n",
      "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained tokenizer inheriting from [`PreTrainedTokenizer`].\n",
      "    \n",
      "            If not provided, the default tokenizer for the given `model` will be loaded (if it is a string). If `model`\n",
      "            is not specified or not a string, then the default tokenizer for `config` is loaded (if it is a string).\n",
      "            However, if `config` is also not given or not a string, then the default tokenizer for the given `task`\n",
      "            will be loaded.\n",
      "        feature_extractor (`str` or [`PreTrainedFeatureExtractor`], *optional*):\n",
      "            The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained feature extractor inheriting from [`PreTrainedFeatureExtractor`].\n",
      "    \n",
      "            Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
      "            models. Multi-modal models will also require a tokenizer to be passed.\n",
      "    \n",
      "            If not provided, the default feature extractor for the given `model` will be loaded (if it is a string). If\n",
      "            `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n",
      "            is a string). However, if `config` is also not given or not a string, then the default feature extractor\n",
      "            for the given `task` will be loaded.\n",
      "        image_processor (`str` or [`BaseImageProcessor`], *optional*):\n",
      "            The image processor that will be used by the pipeline to preprocess images for the model. This can be a\n",
      "            model identifier or an actual image processor inheriting from [`BaseImageProcessor`].\n",
      "    \n",
      "            Image processors are used for Vision models and multi-modal models that require image inputs. Multi-modal\n",
      "            models will also require a tokenizer to be passed.\n",
      "    \n",
      "            If not provided, the default image processor for the given `model` will be loaded (if it is a string). If\n",
      "            `model` is not specified or not a string, then the default image processor for `config` is loaded (if it is\n",
      "            a string).\n",
      "        processor (`str` or [`ProcessorMixin`], *optional*):\n",
      "            The processor that will be used by the pipeline to preprocess data for the model. This can be a model\n",
      "            identifier or an actual processor inheriting from [`ProcessorMixin`].\n",
      "    \n",
      "            Processors are used for multi-modal models that require multi-modal inputs, for example, a model that\n",
      "            requires both text and image inputs.\n",
      "    \n",
      "            If not provided, the default processor for the given `model` will be loaded (if it is a string). If `model`\n",
      "            is not specified or not a string, then the default processor for `config` is loaded (if it is a string).\n",
      "        framework (`str`, *optional*):\n",
      "            The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "            installed.\n",
      "    \n",
      "            If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "            provided.\n",
      "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "            When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
      "            branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
      "            artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n",
      "        use_fast (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n",
      "        use_auth_token (`str` or *bool*, *optional*):\n",
      "            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      "            when running `hf auth login` (stored in `~/.huggingface`).\n",
      "        device (`int` or `str` or `torch.device`):\n",
      "            Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n",
      "            pipeline will be allocated.\n",
      "        device_map (`str` or `dict[str, Union[int, str, torch.device]`, *optional*):\n",
      "            Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate` library is present, set\n",
      "            `device_map=\"auto\"` to compute the most optimized `device_map` automatically (see\n",
      "            [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)\n",
      "            for more information).\n",
      "    \n",
      "            <Tip warning={true}>\n",
      "    \n",
      "            Do not use `device_map` AND `device` at the same time as they will conflict\n",
      "    \n",
      "            </Tip>\n",
      "    \n",
      "        dtype (`str` or `torch.dtype`, *optional*):\n",
      "            Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      "            (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`).\n",
      "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to allow for custom code defined on the Hub in their own modeling, configuration,\n",
      "            tokenization or even pipeline files. This option should only be set to `True` for repositories you trust\n",
      "            and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
      "        model_kwargs (`dict[str, Any]`, *optional*):\n",
      "            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\n",
      "            **model_kwargs)` function.\n",
      "        kwargs (`dict[str, Any]`, *optional*):\n",
      "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
      "            corresponding pipeline class for possible values).\n",
      "    \n",
      "    Returns:\n",
      "        [`Pipeline`]: A suitable pipeline for the task.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    ```python\n",
      "    >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
      "    \n",
      "    >>> # Sentiment analysis pipeline\n",
      "    >>> analyzer = pipeline(\"sentiment-analysis\")\n",
      "    \n",
      "    >>> # Question answering pipeline, specifying the checkpoint identifier\n",
      "    >>> oracle = pipeline(\n",
      "    ...     \"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", tokenizer=\"google-bert/bert-base-cased\"\n",
      "    ... )\n",
      "    \n",
      "    >>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
      "    >>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
      "    >>> recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a7aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\",model =\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b32a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"The tiger (Panthera tigris) is a large cat and a member of the genus Panthera native to Asia. It has a powerful, muscular body with a large head and paws, a long tail and orange fur with black, mostly vertical stripes. It is traditionally classified into nine recent subspecies, though some recognise only two subspecies, mainland Asian tigers and the island tigers of the Sunda Islands.\n",
    "\n",
    "Throughout the tiger's range, it inhabits mainly forests, from coniferous and temperate broadleaf and mixed forests in the Russian Far East and Northeast China to tropical and subtropical moist broadleaf forests on the Indian subcontinent and Southeast Asia. The tiger is an apex predator and preys mainly on ungulates, which it takes by ambush. It lives a mostly solitary life and occupies home ranges, defending these from individuals of the same sex. The range of a male tiger overlaps with that of multiple females with whom he mates. Females give birth to usually two or three cubs that stay with their mother for about two years. When becoming independent, they leave their mother's home range and establish their own.\n",
    "\n",
    "Since the early 20th century, tiger populations have lost at least 93% of their historic range and are locally extinct in West and Central Asia, in large areas of China and on the islands of Java and Bali. Today, the tiger's range is severely fragmented. It is listed as Endangered on the IUCN Red List of Threatened Species, as its range is thought to have declined by 53% to 68% since the late 1990s. Major threats to tigers are habitat destruction and fragmentation due to deforestation, poaching for fur and the illegal trade of body parts for medicinal purposes. Tigers are also victims of humanâ€“wildlife conflict as they attack and prey on livestock in areas where natural prey is scarce. The tiger is legally protected in all range countries. National conservation measures consist of action plans, anti-poaching patrols and schemes for monitoring tiger populations. In several range countries, wildlife corridors have been established and tiger reintroduction is planned.\n",
    "\n",
    "The tiger is among the most popular of the world's charismatic megafauna. It has been kept in captivity since ancient times and has been trained to perform in circuses and other entertainment shows. The tiger featured prominently in the ancient mythology and folklore of cultures throughout its historic range and has continued to appear in culture worldwide.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e6fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary =summarizer(text,max_length = 80, min_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ede456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tiger (Panthera tigris) is a large cat and a member of the genus Panthera native to Asia. It has a powerful, muscular body with a large head and paws, a long tail and orange fur with black, mostly vertical stripes. Tiger populations have lost at least 93% of their historic range and are locally extinct in West and Central Asia.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e994690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dai\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "summarizer1=pipeline(\"summarization\",model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10448f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 =\"\"\"The tiger (Panthera tigris) is a large cat and a member of the genus Panthera native to Asia. It has a powerful, muscular body with a large head and paws, a long tail and orange fur with black, mostly vertical stripes. It is traditionally classified into nine recent subspecies, though some recognise only two subspecies, mainland Asian tigers and the island tigers of the Sunda Islands.\n",
    "\n",
    "Throughout the tiger's range, it inhabits mainly forests, from coniferous and temperate broadleaf and mixed forests in the Russian Far East and Northeast China to tropical and subtropical moist broadleaf forests on the Indian subcontinent and Southeast Asia. The tiger is an apex predator and preys mainly on ungulates, which it takes by ambush. It lives a mostly solitary life and occupies home ranges, defending these from individuals of the same sex. The range of a male tiger overlaps with that of multiple females with whom he mates. Females give birth to usually two or three cubs that stay with their mother for about two years. When becoming independent, they leave their mother's home range and establish their own.\n",
    "\n",
    "Since the early 20th century, tiger populations have lost at least 93% of their historic range and are locally extinct in West and Central Asia, in large areas of China and on the islands of Java and Bali. Today, the tiger's range is severely fragmented. It is listed as Endangered on the IUCN Red List of Threatened Species, as its range is thought to have declined by 53% to 68% since the late 1990s. Major threats to tigers are habitat destruction and fragmentation due to deforestation, poaching for fur and the illegal trade of body parts for medicinal purposes. Tigers are also victims of humanâ€“wildlife conflict as they attack and prey on livestock in areas where natural prey is scarce. The tiger is legally protected in all range countries. National conservation measures consist of action plans, anti-poaching patrols and schemes for monitoring tiger populations. In several range countries, wildlife corridors have been established and tiger reintroduction is planned.\n",
    "\n",
    "The tiger is among the most popular of the world's charismatic megafauna. It has been kept in captivity since ancient times and has been trained to perform in circuses and other entertainment shows. The tiger featured prominently in the ancient mythology and folklore of cultures throughout its historic range and has continued to appear in culture worldwide.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f8858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "summary1 =summarizer1(text,max_length = 80, min_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5c7099c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the tiger is a large cat and a member of the genus Panthera native to Asia . it inhabits mainly forests, from coniferous and temperate broadleaf and mixed forests in the Russian Far East and Northeast China . females give birth to usually two or three cubs that stay with their mother for about two years .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary1[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e97e539c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model Waris01/google-t5-finetuning-text-summarization with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>). See the original errors:\n\nwhile loading with AutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1148, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: Waris01/google-t5-finetuning-text-summarization does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1148, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: Waris01/google-t5-finetuning-text-summarization does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFAutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2857, in from_pretrained\n    raise OSError(\nOSError: Waris01/google-t5-finetuning-text-summarization does not appear to have a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2857, in from_pretrained\n    raise OSError(\nOSError: Waris01/google-t5-finetuning-text-summarization does not appear to have a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summarizer2\u001b[38;5;241m=\u001b[39m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWaris01/google-t5-finetuning-text-summarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1027\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m-> 1027\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m   1028\u001b[0m         adapter_path \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[0;32m   1029\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m   1030\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   1031\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m   1032\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   1033\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m   1034\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1035\u001b[0m     )\n\u001b[0;32m   1037\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py:333\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    332\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    334\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         )\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model Waris01/google-t5-finetuning-text-summarization with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>). See the original errors:\n\nwhile loading with AutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1148, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: Waris01/google-t5-finetuning-text-summarization does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 1148, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: Waris01/google-t5-finetuning-text-summarization does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFAutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2857, in from_pretrained\n    raise OSError(\nOSError: Waris01/google-t5-finetuning-text-summarization does not appear to have a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Users\\dai\\.conda\\envs\\tf_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2857, in from_pretrained\n    raise OSError(\nOSError: Waris01/google-t5-finetuning-text-summarization does not appear to have a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\n\n"
     ]
    }
   ],
   "source": [
    "summarizer2=pipeline(\"summarization\",model=\"Waris01/google-t5-finetuning-text-summarization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
